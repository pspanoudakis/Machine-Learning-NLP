{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQuAD_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F371cZOMmCax",
        "outputId": "9a1718e6-343e-40de-d393-4714dab57934"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UzYM4_DLLGtD"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import Adam\n",
        "\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "\n",
        "from pprint import pprint\n",
        "import textwrap\n",
        "\n",
        "# Wrap text to 80 characters.\n",
        "wrapper = textwrap.TextWrapper(width=80) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Question:\n",
        "    def __init__(self, text, answer, context, isImpossible = False) -> None:\n",
        "        self.text = text\n",
        "        self.context = context\n",
        "        if isImpossible:\n",
        "            self.answer = (-1, -1)\n",
        "        else:\n",
        "            endCharIndex = answer['answer_start'] + len(answer['text']) - 1\n",
        "            whitespacesBeforeAnswer = 0\n",
        "            whitespacesInAnswer = 0\n",
        "            for i in context.whitespaces:\n",
        "                if i >= answer['answer_start']:\n",
        "                    if i < endCharIndex:\n",
        "                        whitespacesInAnswer += 1\n",
        "                    else:\n",
        "                        break\n",
        "                else:\n",
        "                    whitespacesBeforeAnswer += 1\n",
        "            noWhitespaceStart = answer['answer_start'] - whitespacesBeforeAnswer\n",
        "            noWhitespaceEnd = noWhitespaceStart + len(answer['text']) - 1 - whitespacesInAnswer\n",
        "            self.answer = context.getAnswerTokenIds(noWhitespaceStart, noWhitespaceEnd)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return str({\n",
        "            \"text\": self.text,\n",
        "            \"answer_start\": self.answer[0],\n",
        "            \"answer_end\": self.answer[1]\n",
        "        })\n",
        "\n",
        "class QuestionContext:\n",
        "    def __init__(self, text, tokenizer) -> None:\n",
        "        self.text = text\n",
        "        self.tokenIds = tokenizer(text)['input_ids']\n",
        "        self.tokens = tokenizer.convert_ids_to_tokens(self.tokenIds)\n",
        "        whitespaces = []\n",
        "        for i, c in enumerate(text):\n",
        "            if c == ' ':\n",
        "                whitespaces.append(i)\n",
        "        \n",
        "        self.whitespaces = tuple(whitespaces)\n",
        "\n",
        "    def getAnswerTokenIds(self, startCharIndex, endCharIndex):\n",
        "        answerStart = -1\n",
        "        answerEnd = -1\n",
        "        currChar = 0\n",
        "        for index, token in enumerate(self.tokens):\n",
        "            if (index != 0) and (index != len(self.tokens) - 1):\n",
        "                cleanToken = token.replace('##', '')\n",
        "                for c in cleanToken:\n",
        "                    if currChar == startCharIndex:\n",
        "                        answerStart = index\n",
        "                    if currChar == endCharIndex:\n",
        "                        answerEnd = index\n",
        "                        return (answerStart, answerEnd)\n",
        "                    currChar += 1\n"
      ],
      "metadata": {
        "id": "QTAGsLf0ZnH4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82Ao44hql8Ie",
        "outputId": "03d70ea3-741c-4ede-fbc6-c4cbfb6cd8a9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "with open('sample.json') as samplesFile:\n",
        "    samplesRaw = json.load(samplesFile)['data']\n",
        "    for group in samplesRaw:\n",
        "        for paragraph in group['paragraphs']:\n",
        "            context = QuestionContext(paragraph['context'], tokenizer)\n",
        "            for qa in paragraph['qas']:\n",
        "                questions.append(Question(qa['question'], qa['answers'][0], context, qa['is_impossible']))\n",
        "pprint(questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_2J2jMaLj5B",
        "outputId": "120cb131-e129-4c2d-f89a-2b3c71a6cb10"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'text': 'When did Beyonce start becoming popular?', 'answer_start': 67, 'answer_end': 70},\n",
            " {'text': 'What areas did Beyonce compete in when she was growing up?', 'answer_start': 55, 'answer_end': 57},\n",
            " {'text': \"When did Beyonce leave Destiny's Child and become a solo singer?\", 'answer_start': 128, 'answer_end': 128},\n",
            " {'text': 'In what city and state did Beyonce  grow up? ', 'answer_start': 47, 'answer_end': 49},\n",
            " {'text': 'In which decade did Beyonce become famous?', 'answer_start': 69, 'answer_end': 70},\n",
            " {'text': 'In what R&B group was she the lead singer?', 'answer_start': 81, 'answer_end': 84},\n",
            " {'text': 'What album made her a worldwide known artist?', 'answer_start': 124, 'answer_end': 126},\n",
            " {'text': \"Who managed the Destiny's Child group?\", 'answer_start': 91, 'answer_end': 92},\n",
            " {'text': 'When did Beyoncé rise to fame?', 'answer_start': 69, 'answer_end': 70},\n",
            " {'text': \"What role did Beyoncé have in Destiny's Child?\", 'answer_start': 72, 'answer_end': 73},\n",
            " {'text': 'What was the first album Beyoncé released as a solo artist?', 'answer_start': 124, 'answer_end': 126},\n",
            " {'text': 'When did Beyoncé release Dangerously in Love?', 'answer_start': 128, 'answer_end': 128},\n",
            " {'text': 'How many Grammy awards did Beyoncé win for her first solo album?', 'answer_start': 141, 'answer_end': 141},\n",
            " {'text': \"What was Beyoncé's role in Destiny's Child?\", 'answer_start': 72, 'answer_end': 73},\n",
            " {'text': \"What was the name of Beyoncé's first solo album?\", 'answer_start': 124, 'answer_end': 126}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
        "# tokenizer's behavior, let's also get the token strings and display them.\n",
        "input_ids = tokenizer.encode(context.text)\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# For each token and its id...\n",
        "for index, (token, id) in enumerate(zip(tokens, input_ids)):\n",
        "    \n",
        "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    \n",
        "    # Print the token string and its ID in two columns.\n",
        "    print('{}) {:<12} {:>6,}'.format(index, token, id))\n",
        "\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITrNQmeyodP8",
        "outputId": "2599e521-b812-476d-a159-8f464a4667eb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0) [CLS]           101\n",
            "1) beyonce      20,773\n",
            "2) gi           21,025\n",
            "3) ##selle      19,358\n",
            "4) knowles      22,815\n",
            "5) -             1,011\n",
            "6) carter        5,708\n",
            "7) (             1,006\n",
            "8) /             1,013\n",
            "9) bi           12,170\n",
            "10) ##ː          23,432\n",
            "11) ##ˈ          29,715\n",
            "12) ##j           3,501\n",
            "13) ##ɒ          29,678\n",
            "14) ##nse        12,325\n",
            "15) ##ɪ          29,685\n",
            "16) /             1,013\n",
            "17) bee          10,506\n",
            "18) -             1,011\n",
            "19) yo           10,930\n",
            "20) ##n           2,078\n",
            "21) -             1,011\n",
            "22) say           2,360\n",
            "23) )             1,007\n",
            "24) (             1,006\n",
            "25) born          2,141\n",
            "26) september     2,244\n",
            "27) 4             1,018\n",
            "28) ,             1,010\n",
            "29) 1981          3,261\n",
            "30) )             1,007\n",
            "31) is            2,003\n",
            "32) an            2,019\n",
            "33) american      2,137\n",
            "34) singer        3,220\n",
            "35) ,             1,010\n",
            "36) songwriter    6,009\n",
            "37) ,             1,010\n",
            "38) record        2,501\n",
            "39) producer      3,135\n",
            "40) and           1,998\n",
            "41) actress       3,883\n",
            "42) .             1,012\n",
            "43) born          2,141\n",
            "44) and           1,998\n",
            "45) raised        2,992\n",
            "46) in            1,999\n",
            "47) houston       5,395\n",
            "48) ,             1,010\n",
            "49) texas         3,146\n",
            "50) ,             1,010\n",
            "51) she           2,016\n",
            "52) performed     2,864\n",
            "53) in            1,999\n",
            "54) various       2,536\n",
            "55) singing       4,823\n",
            "56) and           1,998\n",
            "57) dancing       5,613\n",
            "58) competitions  6,479\n",
            "59) as            2,004\n",
            "60) a             1,037\n",
            "61) child         2,775\n",
            "62) ,             1,010\n",
            "63) and           1,998\n",
            "64) rose          3,123\n",
            "65) to            2,000\n",
            "66) fame          4,476\n",
            "67) in            1,999\n",
            "68) the           1,996\n",
            "69) late          2,397\n",
            "70) 1990s         4,134\n",
            "71) as            2,004\n",
            "72) lead          2,599\n",
            "73) singer        3,220\n",
            "74) of            1,997\n",
            "75) r             1,054\n",
            "76) &             1,004\n",
            "77) b             1,038\n",
            "78) girl          2,611\n",
            "79) -             1,011\n",
            "80) group         2,177\n",
            "81) destiny      10,461\n",
            "82) '             1,005\n",
            "83) s             1,055\n",
            "84) child         2,775\n",
            "85) .             1,012\n",
            "86) managed       3,266\n",
            "87) by            2,011\n",
            "88) her           2,014\n",
            "89) father        2,269\n",
            "90) ,             1,010\n",
            "91) mathew       25,436\n",
            "92) knowles      22,815\n",
            "93) ,             1,010\n",
            "94) the           1,996\n",
            "95) group         2,177\n",
            "96) became        2,150\n",
            "97) one           2,028\n",
            "98) of            1,997\n",
            "99) the           1,996\n",
            "100) world         2,088\n",
            "101) '             1,005\n",
            "102) s             1,055\n",
            "103) best          2,190\n",
            "104) -             1,011\n",
            "105) selling       4,855\n",
            "106) girl          2,611\n",
            "107) groups        2,967\n",
            "108) of            1,997\n",
            "109) all           2,035\n",
            "110) time          2,051\n",
            "111) .             1,012\n",
            "112) their         2,037\n",
            "113) hiatus       14,221\n",
            "114) saw           2,387\n",
            "115) the           1,996\n",
            "116) release       2,713\n",
            "117) of            1,997\n",
            "118) beyonce      20,773\n",
            "119) '             1,005\n",
            "120) s             1,055\n",
            "121) debut         2,834\n",
            "122) album         2,201\n",
            "123) ,             1,010\n",
            "124) dangerously  20,754\n",
            "125) in            1,999\n",
            "126) love          2,293\n",
            "127) (             1,006\n",
            "128) 2003          2,494\n",
            "129) )             1,007\n",
            "130) ,             1,010\n",
            "131) which         2,029\n",
            "132) established   2,511\n",
            "133) her           2,014\n",
            "134) as            2,004\n",
            "135) a             1,037\n",
            "136) solo          3,948\n",
            "137) artist        3,063\n",
            "138) worldwide     4,969\n",
            "139) ,             1,010\n",
            "140) earned        3,687\n",
            "141) five          2,274\n",
            "142) grammy        8,922\n",
            "143) awards        2,982\n",
            "144) and           1,998\n",
            "145) featured      2,956\n",
            "146) the           1,996\n",
            "147) billboard     4,908\n",
            "148) hot           2,980\n",
            "149) 100           2,531\n",
            "150) number        2,193\n",
            "151) -             1,011\n",
            "152) one           2,028\n",
            "153) singles       3,895\n",
            "154) \"             1,000\n",
            "155) crazy         4,689\n",
            "156) in            1,999\n",
            "157) love          2,293\n",
            "158) \"             1,000\n",
            "159) and           1,998\n",
            "160) \"             1,000\n",
            "161) baby          3,336\n",
            "162) boy           2,879\n",
            "163) \"             1,000\n",
            "164) .             1,012\n",
            "\n",
            "165) [SEP]           102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(wrapper.fill(context.text))"
      ],
      "metadata": {
        "id": "65-qbgboi5Sd"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}