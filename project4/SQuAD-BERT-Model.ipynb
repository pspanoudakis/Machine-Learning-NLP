{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"LEARNING_RATE=5e-5\nEPOCHS=3\nBATCH_SIZE=16\n\nMAX_LENGTH=400","metadata":{"id":"wmeMeTrXUqvn","execution":{"iopub.status.busy":"2022-03-08T18:57:50.126494Z","iopub.execute_input":"2022-03-08T18:57:50.127671Z","iopub.status.idle":"2022-03-08T18:57:50.155893Z","shell.execute_reply.started":"2022-03-08T18:57:50.127542Z","shell.execute_reply":"2022-03-08T18:57:50.155082Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import json\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\n\nfrom transformers import BertTokenizer, BertForQuestionAnswering\nfrom transformers.tokenization_utils_base import PaddingStrategy, TruncationStrategy\n\nfrom pprint import pprint\nimport textwrap\n\n# Wrap text to 80 characters.\nwrapper = textwrap.TextWrapper(width=80) ","metadata":{"id":"UzYM4_DLLGtD","execution":{"iopub.status.busy":"2022-03-08T18:57:50.157931Z","iopub.execute_input":"2022-03-08T18:57:50.158244Z","iopub.status.idle":"2022-03-08T18:57:51.893043Z","shell.execute_reply.started":"2022-03-08T18:57:50.158207Z","shell.execute_reply":"2022-03-08T18:57:51.891873Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"CPU_DEVICE = 'cpu'\nCUDA_DEVICE = 'cuda'\nDEVICE = CUDA_DEVICE if torch.cuda.is_available() else CPU_DEVICE","metadata":{"id":"hkVdzsJK_Lje","execution":{"iopub.status.busy":"2022-03-08T18:57:51.894516Z","iopub.execute_input":"2022-03-08T18:57:51.894779Z","iopub.status.idle":"2022-03-08T18:57:51.952185Z","shell.execute_reply.started":"2022-03-08T18:57:51.894745Z","shell.execute_reply":"2022-03-08T18:57:51.951454Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Question:\n    def __init__(self, text, answer, context, isImpossible = False) -> None:\n        self.text = text\n        self.context = context\n        if isImpossible:\n            self.answer = (-1, -1)\n        else:\n            endCharIndex = answer['answer_start'] + len(answer['text']) - 1\n            whitespacesBeforeAnswer = 0\n            whitespacesInAnswer = 0\n            for i in context.whitespaces:\n                if i >= answer['answer_start']:\n                    if i < endCharIndex:\n                        whitespacesInAnswer += 1\n                    else:\n                        break\n                else:\n                    whitespacesBeforeAnswer += 1\n            noWhitespaceStart = answer['answer_start'] - whitespacesBeforeAnswer\n            noWhitespaceEnd = noWhitespaceStart + len(answer['text']) - 1 - whitespacesInAnswer\n            self.answer = context.getAnswerTokenIndexes(noWhitespaceStart, noWhitespaceEnd)\n\n    def __repr__(self) -> str:\n        return str({\n            \"text\": self.text,\n            \"answer_start\": self.answer[0],\n            \"answer_end\": self.answer[1],\n            #\"answer\": ' '.join(self.context.tokens[self.answer[0]:self.answer[1]+1])\n        })\n\nclass QuestionContext:\n    def __init__(self, text, tokenizer) -> None:\n        self.text = text\n        self.tokenIds = tokenizer(text, truncation=True, max_length=MAX_LENGTH)['input_ids']\n        self.tokens = tokenizer.convert_ids_to_tokens(self.tokenIds)\n        whitespaces = []\n        for i, c in enumerate(text):\n            if c == ' ':\n                whitespaces.append(i)\n        \n        self.whitespaces = tuple(whitespaces)\n\n    def getAnswerTokenIndexes(self, startCharIndex, endCharIndex):\n        answerStart = -1\n        answerEnd = -1\n        currChar = 0\n        for index, token in enumerate(self.tokens):\n            if (index != 0) and (index != len(self.tokens) - 1):\n                cleanToken = token.replace('##', '')\n                for c in cleanToken:\n                    if currChar == startCharIndex:\n                        answerStart = index\n                    if currChar == endCharIndex:\n                        answerEnd = index\n                        return (answerStart, answerEnd)\n                    currChar += 1\n        return (-1, -1)","metadata":{"id":"QTAGsLf0ZnH4","execution":{"iopub.status.busy":"2022-03-08T18:57:51.956475Z","iopub.execute_input":"2022-03-08T18:57:51.956782Z","iopub.status.idle":"2022-03-08T18:57:51.988322Z","shell.execute_reply.started":"2022-03-08T18:57:51.956743Z","shell.execute_reply":"2022-03-08T18:57:51.986762Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"questions = []\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nwith open('../input/squad-20/train-v2.0.json') as samplesFile:\n    samplesRaw = json.load(samplesFile)['data']\n    for group in samplesRaw:\n        for paragraph in group['paragraphs']:\n            context = QuestionContext(paragraph['context'], tokenizer)\n            for qa in paragraph['qas']:\n                #answer = qa['answers'][0] if qa['is_impossible'] else ''\n                #questions.append(Question(qa['question'], answer, context, qa['is_impossible']))\n                if not qa['is_impossible']:\n                    questions.append(Question(qa['question'], qa['answers'][0], context, qa['is_impossible']))\n#pprint(questions)","metadata":{"id":"W_2J2jMaLj5B","outputId":"67a355b4-7e25-49f9-cb10-acd5ec827f71","execution":{"iopub.status.busy":"2022-03-08T18:57:51.989632Z","iopub.execute_input":"2022-03-08T18:57:51.990022Z","iopub.status.idle":"2022-03-08T18:59:33.685633Z","shell.execute_reply.started":"2022-03-08T18:57:51.989986Z","shell.execute_reply":"2022-03-08T18:59:33.684766Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c382206a7dd49e9aa9461005de0b6f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45c9853e33eb4dcba7bebc5ec451a6b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33d0d809753945128550ffb411d1abf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9185a074c34144fba8f1703bd1a475fb"}},"metadata":{}}]},{"cell_type":"code","source":"class QuestionsDataset(torch.utils.data.Dataset):\n    def __init__(self, questions) -> None:\n        super().__init__()\n        self.questions = [q.text for q in questions]\n        self.contexts = [q.context.text for q in questions]\n        self.answers = [torch.tensor(q.answer) for q in questions]\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, index):\n        return self.questions[index], self.contexts[index], self.answers[index]","metadata":{"id":"E7Rs6zeENvJk","execution":{"iopub.status.busy":"2022-03-08T18:59:33.690227Z","iopub.execute_input":"2022-03-08T18:59:33.690482Z","iopub.status.idle":"2022-03-08T18:59:33.705105Z","shell.execute_reply.started":"2022-03-08T18:59:33.690450Z","shell.execute_reply":"2022-03-08T18:59:33.703644Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#ret = tokenizer._batch_encode_plus([['is', 'hi oops'], ['of', 'hello'], ['i am good, thanks', 'haha']], max_length=10, padding_strategy=PaddingStrategy.MAX_LENGTH)\n#for id in ret[\"input_ids\"][2]:\n#    print(tokenizer.convert_ids_to_tokens(id))","metadata":{"id":"dj4uKKl4_k8b","execution":{"iopub.status.busy":"2022-03-08T18:59:33.706330Z","iopub.execute_input":"2022-03-08T18:59:33.706681Z","iopub.status.idle":"2022-03-08T18:59:33.719696Z","shell.execute_reply.started":"2022-03-08T18:59:33.706651Z","shell.execute_reply":"2022-03-08T18:59:33.718934Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"trainDataset = QuestionsDataset(questions)\ntrainSetLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=False)\nmodel = BertForQuestionAnswering.from_pretrained('bert-base-uncased').to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","metadata":{"id":"ZoxQAYdROoKE","outputId":"84a6fa18-5fe8-4a38-b210-a1be36a937d4","execution":{"iopub.status.busy":"2022-03-08T18:59:33.720799Z","iopub.execute_input":"2022-03-08T18:59:33.721237Z","iopub.status.idle":"2022-03-08T18:59:53.861442Z","shell.execute_reply.started":"2022-03-08T18:59:33.721204Z","shell.execute_reply":"2022-03-08T18:59:53.860702Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19ee2048d55447d2ad124334f79e8bb0"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def predictionsF1Score(modelAnswers, trueAnswers):\n\n    def findAnswerF1(modelAnswer, trueAnswer):\n        modelSequence = range(modelAnswer[0], modelAnswer[1] + 1 )\n        trueSequence = range(trueAnswer[0], trueAnswer[1] + 1 )\n        numCommon = len(set(trueSequence).intersection(modelSequence))\n\n        if numCommon == 0:\n            return 0\n        \n        precision = 1.0 * numCommon / len(trueSequence)\n        recall = 1.0 * numCommon / len(modelSequence)\n        f1 = (2 * precision * recall) / (precision + recall)\n        return f1\n\n    totalF1 = 0\n    for model, gold in zip(modelAnswers, trueAnswers):\n        totalF1 += findAnswerF1(model, gold)\n\n    return totalF1/len(trueAnswers)\n\ndef predictionsExactScore(modelAnswers, trueAnswers):\n    correct = 0\n    for model, true in zip(modelAnswers, trueAnswers):\n        correct += int( (model[0] == true[0]) and (model[1] == true[1]) )\n    \n    return correct/len(trueAnswers)\n\ndef getPredictedAnswers(startLogits, endLogits):\n    softmaxStart = torch.log_softmax(startLogits, dim = 1)\n    _, start = torch.max(softmaxStart, dim = 1)\n\n    softmaxEnd = torch.log_softmax(endLogits, dim = 1)\n    _, end = torch.max(softmaxEnd, dim = 1)\n    return (start.cpu().detach().numpy(), end.cpu().detach().numpy())","metadata":{"id":"qZw2MOKoVj-1","execution":{"iopub.status.busy":"2022-03-08T18:59:53.862646Z","iopub.execute_input":"2022-03-08T18:59:53.862923Z","iopub.status.idle":"2022-03-08T18:59:53.876228Z","shell.execute_reply.started":"2022-03-08T18:59:53.862891Z","shell.execute_reply":"2022-03-08T18:59:53.875406Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    model.train()\n    epochExactBatchScores = []\n    epochBatchLosses = []\n    epochBatchF1 = []\n    for batchQuestions, batchContexts, batchAnswers in trainSetLoader:\n        qaPairs = [[question, answer] for question, answer in zip(batchQuestions, batchContexts)]\n        tok = tokenizer._batch_encode_plus( qaPairs,\n                                            truncation_strategy=TruncationStrategy.ONLY_SECOND,\n                                            max_length=MAX_LENGTH,\n                                            padding_strategy=PaddingStrategy.MAX_LENGTH,\n                                            return_tensors=\"pt\")\n        inputIds = tok['input_ids'].to(DEVICE)\n        segmentIds = tok['token_type_ids'].to(DEVICE)\n        attentionMask = tok['attention_mask'].to(DEVICE)\n        startPositions = batchAnswers[:, 0].to(DEVICE)\n        endPositions = batchAnswers[:, 1].to(DEVICE)\n        \n        outputs = model(input_ids=inputIds, token_type_ids=segmentIds, attention_mask=attentionMask, start_positions=startPositions, end_positions=endPositions)    \n        batchLoss = outputs[0]\n\n        optimizer.zero_grad()\n\n        batchLoss.backward()\n\n        optimizer.step()\n\n        startPredictions, endPredictions = getPredictedAnswers(outputs.start_logits, outputs.end_logits)\n        modelAnswers = np.vstack((startPredictions, endPredictions)).T\n        \n        epochExactBatchScores.append(predictionsExactScore(modelAnswers, batchAnswers))\n        epochBatchLosses.append(batchLoss.item())\n        epochBatchF1.append(predictionsF1Score(modelAnswers, batchAnswers))\n    \n    print(f\"############ Epoch {epoch} ############\")\n    print(f\"Exact: {sum(epochExactBatchScores)/len(epochExactBatchScores):.5f}\\\n    F1: {sum(epochBatchF1)/len(epochBatchF1):.5f} Loss: {sum(epochBatchLosses)/len(epochBatchLosses):.5f}\")","metadata":{"id":"EsL_ERhbVUIN","execution":{"iopub.status.busy":"2022-03-08T18:59:53.878573Z","iopub.execute_input":"2022-03-08T18:59:53.879040Z","iopub.status.idle":"2022-03-08T22:17:47.531840Z","shell.execute_reply.started":"2022-03-08T18:59:53.879000Z","shell.execute_reply":"2022-03-08T22:17:47.531111Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"############ Epoch 0 ############\nExact: 0.07195    F1: 0.37757 Loss: 2.91520\n############ Epoch 1 ############\nExact: 0.23869    F1: 0.61149 Loss: 1.85945\n############ Epoch 2 ############\nExact: 0.36434    F1: 0.70873 Loss: 1.42264\n","output_type":"stream"}]}]}