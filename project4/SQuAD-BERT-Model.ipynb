{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"LEARNING_RATE=5e-5\nEPOCHS=3\nBATCH_SIZE=16\n\nMAX_INPUT_LENGTH=400\nMAX_CONTEXT_LENGTH=350\n\nNO_ANSWER = (0, 0)","metadata":{"id":"wmeMeTrXUqvn","execution":{"iopub.status.busy":"2022-03-10T08:21:23.793855Z","iopub.execute_input":"2022-03-10T08:21:23.794428Z","iopub.status.idle":"2022-03-10T08:21:23.818993Z","shell.execute_reply.started":"2022-03-10T08:21:23.794346Z","shell.execute_reply":"2022-03-10T08:21:23.818296Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import json\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\n\nfrom transformers import BertTokenizer, BertForQuestionAnswering\nfrom transformers.tokenization_utils_base import PaddingStrategy, TruncationStrategy\n\n#from pprint import pprint\n#import textwrap\n# Wrap text to 80 characters.\n#wrapper = textwrap.TextWrapper(width=80) ","metadata":{"id":"UzYM4_DLLGtD","execution":{"iopub.status.busy":"2022-03-10T08:21:23.820499Z","iopub.execute_input":"2022-03-10T08:21:23.820786Z","iopub.status.idle":"2022-03-10T08:21:25.268255Z","shell.execute_reply.started":"2022-03-10T08:21:23.820752Z","shell.execute_reply":"2022-03-10T08:21:25.267554Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"CPU_DEVICE = 'cpu'\nCUDA_DEVICE = 'cuda'\nDEVICE = CUDA_DEVICE if torch.cuda.is_available() else CPU_DEVICE","metadata":{"id":"hkVdzsJK_Lje","execution":{"iopub.status.busy":"2022-03-10T08:21:25.269772Z","iopub.execute_input":"2022-03-10T08:21:25.270078Z","iopub.status.idle":"2022-03-10T08:21:25.311236Z","shell.execute_reply.started":"2022-03-10T08:21:25.270042Z","shell.execute_reply":"2022-03-10T08:21:25.310575Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Question:\n    def __init__(self, text, answer, context, isImpossible = False) -> None:\n        self.text = text\n        self.context = context\n        if isImpossible:\n            self.answer = NO_ANSWER\n        else:\n            endCharIndex = answer['answer_start'] + len(answer['text']) - 1\n            whitespacesBeforeAnswer = 0\n            whitespacesInAnswer = 0\n            for i in context.whitespaces:\n                if i >= answer['answer_start']:\n                    if i < endCharIndex:\n                        whitespacesInAnswer += 1\n                    else:\n                        break\n                else:\n                    whitespacesBeforeAnswer += 1\n            noWhitespaceStart = answer['answer_start'] - whitespacesBeforeAnswer\n            noWhitespaceEnd = noWhitespaceStart + len(answer['text']) - 1 - whitespacesInAnswer\n            self.answer = context.getAnswerTokenIndexes(noWhitespaceStart, noWhitespaceEnd)\n\n    def __repr__(self) -> str:\n        if self.answer == NO_ANSWER:            \n            answer = ' '.join(self.context.tokens[self.answer[0]:self.answer[1]+1])\n        else:\n            answer = ''\n        return str({\n            \"text\": self.text,\n            \"answer_start\": self.answer[0],\n            \"answer_end\": self.answer[1],\n            \"answer\": answer\n        })\n\nclass QuestionContext:\n    def __init__(self, text, tokenizer) -> None:\n        self.text = text\n        self.tokenIds = tokenizer(text, truncation=True, max_length=MAX_CONTEXT_LENGTH)['input_ids']\n        self.tokens = tokenizer.convert_ids_to_tokens(self.tokenIds)\n        whitespaces = []\n        for i, c in enumerate(text):\n            if c == ' ':\n                whitespaces.append(i)\n        \n        self.whitespaces = tuple(whitespaces)\n\n    def getAnswerTokenIndexes(self, startCharIndex, endCharIndex):\n        answerStart = -1\n        answerEnd = -1\n        currChar = 0\n        for index, token in enumerate(self.tokens):\n            if (index != 0) and (index != len(self.tokens) - 1):\n                cleanToken = token.replace('##', '')\n                for c in cleanToken:\n                    if currChar == startCharIndex:\n                        answerStart = index\n                    if currChar == endCharIndex:\n                        answerEnd = index\n                        return (answerStart, answerEnd)\n                    currChar += 1\n        return NO_ANSWER","metadata":{"id":"QTAGsLf0ZnH4","execution":{"iopub.status.busy":"2022-03-10T08:21:25.313465Z","iopub.execute_input":"2022-03-10T08:21:25.313966Z","iopub.status.idle":"2022-03-10T08:21:25.329224Z","shell.execute_reply.started":"2022-03-10T08:21:25.313927Z","shell.execute_reply":"2022-03-10T08:21:25.328381Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"questions = []\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nwith open('../input/squad-20/train-v2.0.json') as samplesFile:\n    samplesRaw = json.load(samplesFile)['data']\n    for group in samplesRaw:\n        for paragraph in group['paragraphs']:\n            context = QuestionContext(paragraph['context'], tokenizer)\n            for qa in paragraph['qas']:\n                answer = qa['answers'][0] if not qa['is_impossible'] else None\n                questions.append(Question(qa['question'], answer, context, qa['is_impossible']))\n                #if not qa['is_impossible']:\n                    #questions.append(Question(qa['question'], qa['answers'][0], context, qa['is_impossible']))","metadata":{"id":"W_2J2jMaLj5B","outputId":"67a355b4-7e25-49f9-cb10-acd5ec827f71","execution":{"iopub.status.busy":"2022-03-10T08:21:25.331801Z","iopub.execute_input":"2022-03-10T08:21:25.332002Z","iopub.status.idle":"2022-03-10T08:23:06.292592Z","shell.execute_reply.started":"2022-03-10T08:21:25.331974Z","shell.execute_reply":"2022-03-10T08:23:06.291864Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51c9a080b5ea437aacd63c5047af45ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"654151b823b24446aeed4c5768bcbd06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4779b90cdf984232b795182c8ae5a71e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94c2a858970e40ffa86498cc67afc62e"}},"metadata":{}}]},{"cell_type":"code","source":"class QuestionsDataset(torch.utils.data.Dataset):\n    def __init__(self, questions) -> None:\n        super().__init__()\n        self.questions = [q.text for q in questions]\n        self.contexts = [q.context.text for q in questions]\n        self.answers = [torch.tensor(q.answer) for q in questions]\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, index):\n        return self.questions[index], self.contexts[index], self.answers[index]","metadata":{"id":"E7Rs6zeENvJk","execution":{"iopub.status.busy":"2022-03-10T08:23:06.293964Z","iopub.execute_input":"2022-03-10T08:23:06.294225Z","iopub.status.idle":"2022-03-10T08:23:06.300880Z","shell.execute_reply.started":"2022-03-10T08:23:06.294193Z","shell.execute_reply":"2022-03-10T08:23:06.300115Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"trainDataset = QuestionsDataset(questions)\ntrainSetLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=False)\nmodel = BertForQuestionAnswering.from_pretrained('bert-base-uncased').to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","metadata":{"id":"ZoxQAYdROoKE","outputId":"84a6fa18-5fe8-4a38-b210-a1be36a937d4","execution":{"iopub.status.busy":"2022-03-10T08:23:06.302024Z","iopub.execute_input":"2022-03-10T08:23:06.302745Z","iopub.status.idle":"2022-03-10T08:23:24.636946Z","shell.execute_reply.started":"2022-03-10T08:23:06.302699Z","shell.execute_reply":"2022-03-10T08:23:24.636202Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6905614fc2f34b12b34c5b0469036dfe"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def predictionsF1Score(modelAnswers, trueAnswers):\n\n    def findAnswerF1(modelAnswer, trueAnswer):\n        modelSequence = range(modelAnswer[0], modelAnswer[1] + 1 )\n        trueSequence = range(trueAnswer[0], trueAnswer[1] + 1 )\n        numCommon = len(set(trueSequence).intersection(modelSequence))\n\n        if numCommon == 0:\n            return 0\n        \n        precision = 1.0 * numCommon / len(trueSequence)\n        recall = 1.0 * numCommon / len(modelSequence)\n        f1 = (2 * precision * recall) / (precision + recall)\n        return f1\n\n    totalF1 = 0\n    for model, gold in zip(modelAnswers, trueAnswers):\n        totalF1 += findAnswerF1(model, gold)\n\n    return totalF1/len(trueAnswers)\n\ndef predictionsExactScore(modelAnswers, trueAnswers):\n    correct = 0\n    for model, true in zip(modelAnswers, trueAnswers):\n        correct += int( (model[0] == true[0]) and (model[1] == true[1]) )\n    \n    return correct/len(trueAnswers)\n\ndef getPredictedAnswers(startLogits, endLogits):\n    softmaxStart = torch.log_softmax(startLogits, dim = 1)\n    _, start = torch.max(softmaxStart, dim = 1)\n\n    softmaxEnd = torch.log_softmax(endLogits, dim = 1)\n    _, end = torch.max(softmaxEnd, dim = 1)\n    return (start.cpu().detach().numpy(), end.cpu().detach().numpy())","metadata":{"id":"qZw2MOKoVj-1","execution":{"iopub.status.busy":"2022-03-10T08:23:24.638164Z","iopub.execute_input":"2022-03-10T08:23:24.638415Z","iopub.status.idle":"2022-03-10T08:23:24.650368Z","shell.execute_reply.started":"2022-03-10T08:23:24.638382Z","shell.execute_reply":"2022-03-10T08:23:24.649713Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    model.train()\n    epochExactBatchScores = []\n    epochBatchLosses = []\n    epochBatchF1 = []\n    for batchQuestions, batchContexts, batchAnswers in trainSetLoader:\n        qaPairs = [[question, answer] for question, answer in zip(batchQuestions, batchContexts)]\n        tok = tokenizer._batch_encode_plus( qaPairs,\n                                            truncation_strategy=TruncationStrategy.ONLY_SECOND,\n                                            max_length=MAX_INPUT_LENGTH,\n                                            padding_strategy=PaddingStrategy.MAX_LENGTH,\n                                            return_tensors=\"pt\")\n        inputIds = tok['input_ids'].to(DEVICE)\n        segmentIds = tok['token_type_ids'].to(DEVICE)\n        attentionMask = tok['attention_mask'].to(DEVICE)\n        startPositions = batchAnswers[:, 0].to(DEVICE)\n        endPositions = batchAnswers[:, 1].to(DEVICE)\n        \n        outputs = model(input_ids=inputIds, token_type_ids=segmentIds, attention_mask=attentionMask, start_positions=startPositions, end_positions=endPositions)    \n        \n        optimizer.zero_grad()\n\n        outputs.loss.backward()\n\n        optimizer.step()\n\n        startPredictions, endPredictions = getPredictedAnswers(outputs.start_logits, outputs.end_logits)\n        modelAnswers = np.vstack((startPredictions, endPredictions)).T\n        \n        epochExactBatchScores.append(predictionsExactScore(modelAnswers, batchAnswers))\n        epochBatchLosses.append(outputs.loss.item())\n        epochBatchF1.append(predictionsF1Score(modelAnswers, batchAnswers))\n    \n    print(f\"############ Epoch {epoch} ############\")\n    print(f\"Exact: {sum(epochExactBatchScores)/len(epochExactBatchScores):.5f} \\\n\\    F1: {sum(epochBatchF1)/len(epochBatchF1):.5f} Loss: {sum(epochBatchLosses)/len(epochBatchLosses):.5f}\")","metadata":{"id":"EsL_ERhbVUIN","execution":{"iopub.status.busy":"2022-03-10T08:23:24.651557Z","iopub.execute_input":"2022-03-10T08:23:24.651973Z","iopub.status.idle":"2022-03-10T13:20:37.596890Z","shell.execute_reply.started":"2022-03-10T08:23:24.651938Z","shell.execute_reply":"2022-03-10T13:20:37.596166Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"############ Epoch 0 ############\nExact: 0.31415 \\    F1: 0.40685 Loss: 2.55958\n############ Epoch 1 ############\nExact: 0.38273 \\    F1: 0.55976 Loss: 1.71908\n############ Epoch 2 ############\nExact: 0.47538 \\    F1: 0.66463 Loss: 1.31263\n","output_type":"stream"}]}]}