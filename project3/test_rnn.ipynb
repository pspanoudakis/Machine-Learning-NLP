{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip ./glove.6B.zip"
      ],
      "metadata": {
        "id": "OnPiCMYXBbin"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "f2_ciNnNbl4P"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Iterable, List, Dict, Tuple\n",
        "from numbers import Number\n",
        "import re\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
        "\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, \\\n",
        "                            roc_curve, auc, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "f0FlIb_BiLd9"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "_ = torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "o4VGS3Dtbl4S"
      },
      "outputs": [],
      "source": [
        "TRAIN_SET_PATH = \"./vaccine_train_set.csv\"\n",
        "VALIDATION_SET_PATH = \"./vaccine_validation_set.csv\"\n",
        "EMBEDDINGS_PATH = './glove.6B.50d.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "LCZMyFwLbl4S"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 3\n",
        "CLASS_NAMES = ['Neutral', 'Anti-Vaccine', 'Pro-Vaccine']\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.002\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3svwaenBZsK",
        "outputId": "7a179923-6ce5-4024-b97d-84ef9d064e99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0    0\n",
              "tweet         0\n",
              "label         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ],
      "source": [
        "trainDF = pd.read_csv(TRAIN_SET_PATH)\n",
        "trainDF.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg6GNXGzBZsL",
        "outputId": "3998413f-f3e3-4332-ba69-cabd05f9af3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0    0\n",
              "tweet         0\n",
              "label         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ],
      "source": [
        "validDF = pd.read_csv(VALIDATION_SET_PATH)\n",
        "validDF.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "DyPolYlRBZsM"
      },
      "outputs": [],
      "source": [
        "validLabels = validDF['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "EBtXCW-kbl4T"
      },
      "outputs": [],
      "source": [
        "def createWordVectors(embeddingsPath: str):\n",
        "    wordVectors: Dict[str, np.ndarray] = {}\n",
        "    lineElements: List[str] = []\n",
        "    with open(embeddingsPath) as file:\n",
        "        # Read file line by line\n",
        "        for line in file:\n",
        "            # Remove new line and split\n",
        "            lineElements = line.replace('\\n', '').split()\n",
        "            # The first element in the line is the target word\n",
        "            word = lineElements.pop(0)\n",
        "            # The other elements represent the vector of the word\n",
        "            wordVector = np.array([float(w) for w in lineElements])\n",
        "            # Store the vector for this word\n",
        "            wordVectors[word] = wordVector\n",
        "    dimensions = len(lineElements)\n",
        "\n",
        "    return wordVectors, dimensions\n",
        "\n",
        "wordVectors, dimensions = createWordVectors(EMBEDDINGS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "Rj3-_vFJbl4T"
      },
      "outputs": [],
      "source": [
        "def customPreprocessor(text: str):    \n",
        "    # remove url's\n",
        "    trimmedText = re.sub(r'https?://\\S+|www\\.\\S+|#', '', text).lower()\n",
        "\n",
        "    # remove @ mentions and numbers\n",
        "    res = list()\n",
        "    wait_whitespace = False\n",
        "    for c in trimmedText:\n",
        "        if wait_whitespace:\n",
        "            if c == \" \":\n",
        "                wait_whitespace = False\n",
        "            continue\n",
        "        elif re.match(\"[0-9]\", c) or c == \"@\":\n",
        "            wait_whitespace = True\n",
        "            continue            \n",
        "        res.append(c)\n",
        "    \n",
        "    return ''.join(res)\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "def customTokenizer(text: str):\n",
        "    return tokenizer.tokenize(customPreprocessor(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "zPrYwJbgbl4U"
      },
      "outputs": [],
      "source": [
        "# nltk.TweetTokenizer is used for tweet tokenization\n",
        "\n",
        "def vectorizeTweet(tweet: str, preprocessor: Callable[[str], str], wordVectors: Dict[str, np.ndarray], dimensions: int) -> np.ndarray:\n",
        "    # Split the tweet into words/tokens\n",
        "    words = tokenizer.tokenize(preprocessor(tweet))\n",
        "    # words = tokenizer.tokenize(tweet)\n",
        "\n",
        "    # The sum of the vectors of the tweet words is stored here\n",
        "    vector: np.ndarray = np.zeros(dimensions)\n",
        "    for word in words:\n",
        "        # Get the word/token pre-trained vector\n",
        "        wordVector = wordVectors.get(word)\n",
        "        if wordVector is not None:\n",
        "            # If found, add to the tweet vector\n",
        "            vector += wordVector\n",
        "    \n",
        "    # return the mean vector\n",
        "    return vector / len(words)\n",
        "\n",
        "def vectorizeDataSet(dataSet: Iterable[str], dimensions: int, wordVectors: Dict[str, np.ndarray]):\n",
        "    matrix: np.ndarray = np.zeros(shape=(len(dataSet), dimensions))\n",
        "    for i, sample in enumerate(dataSet):\n",
        "        matrix[i] = vectorizeTweet(sample, customPreprocessor, wordVectors, dimensions)\n",
        "    \n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "hgQ46dIcbl4V"
      },
      "outputs": [],
      "source": [
        "labelField = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "tweetField = Field(tokenize=customTokenizer, include_lengths=True, batch_first=True)\n",
        "fields = [('', None), ('tweet', tweetField), ('label', labelField)]\n",
        "\n",
        "trainDataset = TabularDataset(path=TRAIN_SET_PATH, format='CSV', fields=fields, skip_header=True,)\n",
        "validDataset = TabularDataset(path=VALIDATION_SET_PATH, format='CSV', fields=fields, skip_header=True)\n",
        "\n",
        "trainIterator = BucketIterator(trainDataset, batch_size=BATCH_SIZE)\n",
        "validIterator = BucketIterator(validDataset, batch_size=len(validLabels))\n",
        "\n",
        "\n",
        "tweetField.build_vocab(trainDataset)\n",
        "paddingIndex = tweetField.vocab.stoi['<pad>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "bPQXFtARbl4V"
      },
      "outputs": [],
      "source": [
        "matrixNumWords = len(tweetField.vocab)\n",
        "embeddingsMatrix = torch.zeros(matrixNumWords, dimensions, dtype=torch.float)\n",
        "\n",
        "for i, word in enumerate(tweetField.vocab.itos):\n",
        "    wordVector = wordVectors.get(word)\n",
        "\n",
        "    if wordVector is not None:\n",
        "        embeddingsMatrix[i] = torch.from_numpy(wordVector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "9u1OkBcKbl4W"
      },
      "outputs": [],
      "source": [
        "def calculateAccuracy(predictedLabels: Iterable[Number], trueLabels: Iterable[Number]) -> float:    \n",
        "    correct = 0\n",
        "    for pred, true in zip(predictedLabels, trueLabels):\n",
        "        correct += int(pred == true)\n",
        "    \n",
        "    return correct/len(trueLabels)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "6VguOx9Lbl4X"
      },
      "outputs": [],
      "source": [
        "def getPredictedLabels(predictions: torch.Tensor) -> np.ndarray:\n",
        "    softmaxLayerOut = torch.log_softmax(predictions, dim = 1)\n",
        "    _, labels = torch.max(softmaxLayerOut, dim = 1)\n",
        "    return labels.detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "x9lKXXVWbl4X"
      },
      "outputs": [],
      "source": [
        "class SkipRNN(nn.Module):\n",
        "    def __init__(self, embeddingsMatrix, vectorDimension: int, numLayers: int, hiddenSize: int, skipConnections: bool = True) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.USE_SKIP = skipConnections\n",
        "        self.vectorDimension = vectorDimension\n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddingsMatrix)\n",
        "        \n",
        "        self.startingLayer = nn.LSTM(   input_size=vectorDimension,\n",
        "                                        hidden_size=hiddenSize,\n",
        "                                        num_layers=1,\n",
        "                                        batch_first=True,\n",
        "                                        bidirectional=False )\n",
        "        self.lstmLayers = nn.ModuleList()\n",
        "        for _ in range(numLayers - 1):\n",
        "            self.lstmLayers.append(nn.LSTM( input_size=hiddenSize,\n",
        "                                            hidden_size=hiddenSize,\n",
        "                                            num_layers=1,\n",
        "                                            batch_first=True,\n",
        "                                            bidirectional=False))\n",
        "\n",
        "        self.linear = nn.Linear(hiddenSize, NUM_CLASSES)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "        if (skipConnections):\n",
        "            self.skip = nn.Identity()\n",
        "\n",
        "    def forward(self, input, inputLengths):\n",
        "        embout = self.embedding(input)\n",
        "        #packedInput = pack_padded_sequence(embout, inputLengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        #nextInput, _ = pad_packed_sequence(self.startingLayer(packedInput))\n",
        "        nextInput, _ = self.startingLayer(embout)\n",
        "        nextSkipInput = nextInput\n",
        "        for layer in self.lstmLayers:\n",
        "            output, _ = layer(nextInput)\n",
        "            if self.USE_SKIP and (nextSkipInput is not None):\n",
        "                nextInput = torch.add(output, self.skip(nextSkipInput))\n",
        "                nextInput = output\n",
        "                nextSkipInput = None\n",
        "            else:\n",
        "                nextSkipInput = output\n",
        "                nextInput = output\n",
        "        \n",
        "        #lstmOutput, _ = pad_packed_sequence(nextInput)\n",
        "        #lstmOutput = output\n",
        "        #return self.linear(lstmOutput)\n",
        "        lstmOutput = output[:, -1, :]\n",
        "        classOutput = self.linear(lstmOutput)\n",
        "        #return self.activation(classOutput)\n",
        "        return classOutput\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "lSssRYvZbl4Y"
      },
      "outputs": [],
      "source": [
        "model = SkipRNN(embeddingsMatrix, dimensions, numLayers=4, hiddenSize=16, skipConnections=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "MqI5uI7Ebl4Y"
      },
      "outputs": [],
      "source": [
        "lossFunction = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oduh4Y_bbl4Z",
        "outputId": "aebb19d5-3f03-4245-f8fa-c10ff5b10170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 Train Acc = 46.76731 Valid Acc = 47.72130\n",
            "Epoch   1 Train Acc = 51.40962 Valid Acc = 59.37774\n",
            "Epoch   2 Train Acc = 60.53702 Valid Acc = 63.84750\n",
            "Epoch   3 Train Acc = 63.67212 Valid Acc = 55.69676\n",
            "Epoch   4 Train Acc = 63.97115 Valid Acc = 65.16214\n",
            "Epoch   5 Train Acc = 65.24183 Valid Acc = 64.59246\n",
            "Epoch   6 Train Acc = 66.16971 Valid Acc = 65.38124\n",
            "Epoch   7 Train Acc = 66.75769 Valid Acc = 65.99474\n",
            "Epoch   8 Train Acc = 66.96442 Valid Acc = 66.65206\n",
            "Epoch   9 Train Acc = 67.77356 Valid Acc = 66.91499\n",
            "Epoch  10 Train Acc = 67.88942 Valid Acc = 66.21385\n",
            "Epoch  11 Train Acc = 67.00433 Valid Acc = 66.17003\n",
            "Epoch  12 Train Acc = 67.40288 Valid Acc = 64.06661\n",
            "Epoch  13 Train Acc = 67.53365 Valid Acc = 67.70377\n",
            "Epoch  14 Train Acc = 68.05913 Valid Acc = 67.04645\n",
            "Epoch  15 Train Acc = 67.83990 Valid Acc = 67.61613\n",
            "Epoch  16 Train Acc = 68.39471 Valid Acc = 68.22962\n",
            "Epoch  17 Train Acc = 68.38990 Valid Acc = 67.87905\n",
            "Epoch  18 Train Acc = 67.47404 Valid Acc = 66.65206\n",
            "Epoch  19 Train Acc = 67.83942 Valid Acc = 68.09816\n"
          ]
        }
      ],
      "source": [
        "# Loss, F1 Score & Total Predictions after every epoch are stored here\n",
        "validPredictions: np.ndarray\n",
        "validF1: np.ndarray = np.empty(EPOCHS, dtype=float)\n",
        "validErrors: np.ndarray = np.empty(EPOCHS, dtype=float)\n",
        "validOutput: torch.Tensor\n",
        "\n",
        "trainF1: np.ndarray = np.empty(EPOCHS, dtype=float)\n",
        "trainErrors: np.ndarray = np.empty(EPOCHS, dtype=float)\n",
        "epochTrainPredictions: List[int]\n",
        "\n",
        "# Epochs loop\n",
        "for epoch in range(EPOCHS):\n",
        "    # Set model to train mode\n",
        "    epochLabels = []\n",
        "    epochTrainPredictions = []\n",
        "    batchLosses = []\n",
        "    batchAccs = []\n",
        "    model.train()\n",
        "\n",
        "    # Batch loop\n",
        "    for ((tweets, tweetsLen), labels), _ in trainIterator:\n",
        "        \n",
        "        # Make predictions for batch samples\n",
        "        predictions = model(tweets, tweetsLen)\n",
        "\n",
        "        # Extract & store predicted labels and calculate accuracy\n",
        "        predictedLabels = getPredictedLabels(predictions)\n",
        "        epochTrainPredictions.extend(predictedLabels)\n",
        "        batchAccs.append(calculateAccuracy(predictedLabels, labels))\n",
        "\n",
        "        # Run loss function, store loss & backpropagate\n",
        "        batchLoss = lossFunction(predictions, labels.long())\n",
        "        batchLosses.append(batchLoss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batchLoss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        epochLabels.extend(labels)\n",
        "        # Set model to evaluation mode\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for ((tweets, tweetsLen), labels), _ in validIterator:\n",
        "          # Make predictions on the Validation set\n",
        "          validOutput = model(tweets, tweetsLen)\n",
        "\n",
        "          # Run loss function & store loss\n",
        "          validLoss = lossFunction(validOutput, labels.long())\n",
        "          validErrors[epoch] = validLoss.item()\n",
        "          \n",
        "          # Extract & store predicted labels, calculate accuracy and F1 Score\n",
        "          validPredictions = getPredictedLabels(validOutput)\n",
        "          acc = calculateAccuracy(validPredictions, labels)\n",
        "          validF1[epoch] = f1_score(labels, validPredictions, average=\"micro\")\n",
        "\n",
        "    # Find the total epoch loss & F1 Score for the Train set\n",
        "    trainErrors[epoch] = sum(batchLosses)/len(batchLosses)\n",
        "    trainF1[epoch] = f1_score(epochLabels, epochTrainPredictions, average=\"micro\")\n",
        "    \n",
        "    print(f\"Epoch {epoch:3} Train Acc = {sum(batchAccs)/len(batchAccs):.5f} Valid Acc = {acc:.5f}\\r\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "test_rnn.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}