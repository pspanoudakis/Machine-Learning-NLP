{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable, List, Dict, Tuple\n",
    "from numbers import Number\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET_PATH = \"./vaccine_train_set.csv\"\n",
    "VALIDATION_SET_PATH = \"./vaccine_validation_set.csv\"\n",
    "EMBEDDINGS_PATH = '/mnt/c/Users/pavlo/Downloads/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordVectors(embeddingsPath: str):\n",
    "    wordVectors: Dict[str, np.ndarray] = {}\n",
    "    lineElements: List[str] = []\n",
    "    with open(embeddingsPath) as file:\n",
    "        # Read file line by line\n",
    "        for line in file:\n",
    "            # Remove new line and split\n",
    "            lineElements = line.replace('\\n', '').split()\n",
    "            # The first element in the line is the target word\n",
    "            word = lineElements.pop(0)\n",
    "            # The other elements represent the vector of the word\n",
    "            wordVector = np.array([float(w) for w in lineElements])\n",
    "            # Store the vector for this word\n",
    "            wordVectors[word] = wordVector\n",
    "    dimensions = len(lineElements)\n",
    "\n",
    "    return wordVectors, dimensions\n",
    "\n",
    "wordVectors, dimensions = createWordVectors(EMBEDDINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customPreprocessor(text: str):    \n",
    "    # remove url's\n",
    "    trimmedText = re.sub(r'https?://\\S+|www\\.\\S+|#', '', text).lower()\n",
    "\n",
    "    # remove @ mentions and numbers\n",
    "    res = list()\n",
    "    wait_whitespace = False\n",
    "    for c in trimmedText:\n",
    "        if wait_whitespace:\n",
    "            if c == \" \":\n",
    "                wait_whitespace = False\n",
    "            continue\n",
    "        elif re.match(\"[0-9]\", c) or c == \"@\":\n",
    "            wait_whitespace = True\n",
    "            continue            \n",
    "        res.append(c)\n",
    "    \n",
    "    return ''.join(res)\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "def customTokenizer(text: str):\n",
    "    return tokenizer.tokenize(customPreprocessor(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.TweetTokenizer is used for tweet tokenization\n",
    "\n",
    "def vectorizeTweet(tweet: str, preprocessor: Callable[[str], str], wordVectors: Dict[str, np.ndarray], dimensions: int) -> np.ndarray:\n",
    "    # Split the tweet into words/tokens\n",
    "    words = tokenizer.tokenize(preprocessor(tweet))\n",
    "    # words = tokenizer.tokenize(tweet)\n",
    "\n",
    "    # The sum of the vectors of the tweet words is stored here\n",
    "    vector: np.ndarray = np.zeros(dimensions)\n",
    "    for word in words:\n",
    "        # Get the word/token pre-trained vector\n",
    "        wordVector = wordVectors.get(word)\n",
    "        if wordVector is not None:\n",
    "            # If found, add to the tweet vector\n",
    "            vector += wordVector\n",
    "    \n",
    "    # return the mean vector\n",
    "    return vector / len(words)\n",
    "\n",
    "def vectorizeDataSet(dataSet: Iterable[str], dimensions: int, wordVectors: Dict[str, np.ndarray]):\n",
    "    matrix: np.ndarray = np.zeros(shape=(len(dataSet), dimensions))\n",
    "    for i, sample in enumerate(dataSet):\n",
    "        matrix[i] = vectorizeTweet(sample, customPreprocessor, wordVectors, dimensions)\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelField = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "tweetField = Field(tokenize=customTokenizer, include_lengths=True, batch_first=True)\n",
    "fields = [('', None), ('tweet', tweetField), ('label', labelField)]\n",
    "\n",
    "trainDataset = TabularDataset(path=TRAIN_SET_PATH, format='CSV', fields=fields, skip_header=True,)\n",
    "validDataset = TabularDataset(path=VALIDATION_SET_PATH, format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "trainIterator = BucketIterator(trainDataset, batch_size=32)\n",
    "validIterator = BucketIterator(validDataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20296"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetField.build_vocab(trainDataset)\n",
    "len(tweetField.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixNumWords = len(tweetField.vocab)\n",
    "embeddingsMatrix = torch.zeros(matrixNumWords, dimensions, dtype=torch.float)\n",
    "\n",
    "for i, word in enumerate(tweetField.vocab.itos):\n",
    "    wordVector = wordVectors.get(word)\n",
    "\n",
    "    if wordVector is not None:\n",
    "        embeddingsMatrix[i] = torch.from_numpy(wordVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetField.vocab.stoi['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding.from_pretrained(embeddingsMatrix)\n",
    "# emb(torch.LongTensor([tweetField.vocab.stoi]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ((tweets, tweetsLen), labels), _ in trainIterator:\n",
    "    # tweets: Tensor with the tweets\n",
    "    # each tweet is a Tensor with the corresponding word indexes as values\n",
    "\n",
    "    # tweetsLen: Tensor, contains the length of the corresponding index tweet in `tweets`\n",
    "\n",
    "    embout = emb(tweets)\n",
    "    # embout: Tensor.\n",
    "    # Each element is a 2-D Tensor of a tweet. Each Tensor in it is the vector of the corresponding word.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedRNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
