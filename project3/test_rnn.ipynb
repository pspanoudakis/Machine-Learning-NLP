{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable, List, Dict, Tuple\n",
    "from numbers import Number\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, \\\n",
    "                            roc_curve, auc, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET_PATH = \"./vaccine_train_set.csv\"\n",
    "VALIDATION_SET_PATH = \"./vaccine_validation_set.csv\"\n",
    "EMBEDDINGS_PATH = '/mnt/c/Users/pavlo/Downloads/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "CLASS_NAMES = ['Neutral', 'Anti-Vaccine', 'Pro-Vaccine']\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordVectors(embeddingsPath: str):\n",
    "    wordVectors: Dict[str, np.ndarray] = {}\n",
    "    lineElements: List[str] = []\n",
    "    with open(embeddingsPath) as file:\n",
    "        # Read file line by line\n",
    "        for line in file:\n",
    "            # Remove new line and split\n",
    "            lineElements = line.replace('\\n', '').split()\n",
    "            # The first element in the line is the target word\n",
    "            word = lineElements.pop(0)\n",
    "            # The other elements represent the vector of the word\n",
    "            wordVector = np.array([float(w) for w in lineElements])\n",
    "            # Store the vector for this word\n",
    "            wordVectors[word] = wordVector\n",
    "    dimensions = len(lineElements)\n",
    "\n",
    "    return wordVectors, dimensions\n",
    "\n",
    "wordVectors, dimensions = createWordVectors(EMBEDDINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customPreprocessor(text: str):    \n",
    "    # remove url's\n",
    "    trimmedText = re.sub(r'https?://\\S+|www\\.\\S+|#', '', text).lower()\n",
    "\n",
    "    # remove @ mentions and numbers\n",
    "    res = list()\n",
    "    wait_whitespace = False\n",
    "    for c in trimmedText:\n",
    "        if wait_whitespace:\n",
    "            if c == \" \":\n",
    "                wait_whitespace = False\n",
    "            continue\n",
    "        elif re.match(\"[0-9]\", c) or c == \"@\":\n",
    "            wait_whitespace = True\n",
    "            continue            \n",
    "        res.append(c)\n",
    "    \n",
    "    return ''.join(res)\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "def customTokenizer(text: str):\n",
    "    return tokenizer.tokenize(customPreprocessor(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.TweetTokenizer is used for tweet tokenization\n",
    "\n",
    "def vectorizeTweet(tweet: str, preprocessor: Callable[[str], str], wordVectors: Dict[str, np.ndarray], dimensions: int) -> np.ndarray:\n",
    "    # Split the tweet into words/tokens\n",
    "    words = tokenizer.tokenize(preprocessor(tweet))\n",
    "    # words = tokenizer.tokenize(tweet)\n",
    "\n",
    "    # The sum of the vectors of the tweet words is stored here\n",
    "    vector: np.ndarray = np.zeros(dimensions)\n",
    "    for word in words:\n",
    "        # Get the word/token pre-trained vector\n",
    "        wordVector = wordVectors.get(word)\n",
    "        if wordVector is not None:\n",
    "            # If found, add to the tweet vector\n",
    "            vector += wordVector\n",
    "    \n",
    "    # return the mean vector\n",
    "    return vector / len(words)\n",
    "\n",
    "def vectorizeDataSet(dataSet: Iterable[str], dimensions: int, wordVectors: Dict[str, np.ndarray]):\n",
    "    matrix: np.ndarray = np.zeros(shape=(len(dataSet), dimensions))\n",
    "    for i, sample in enumerate(dataSet):\n",
    "        matrix[i] = vectorizeTweet(sample, customPreprocessor, wordVectors, dimensions)\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelField = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "tweetField = Field(tokenize=customTokenizer, include_lengths=True, batch_first=True)\n",
    "fields = [('', None), ('tweet', tweetField), ('label', labelField)]\n",
    "\n",
    "trainDataset = TabularDataset(path=TRAIN_SET_PATH, format='CSV', fields=fields, skip_header=True,)\n",
    "validDataset = TabularDataset(path=VALIDATION_SET_PATH, format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "trainIterator = BucketIterator(trainDataset, batch_size=32)\n",
    "validIterator = BucketIterator(validDataset, batch_size=32)\n",
    "\n",
    "tweetField.build_vocab(trainDataset)\n",
    "paddingIndex = tweetField.vocab.stoi['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixNumWords = len(tweetField.vocab)\n",
    "embeddingsMatrix = torch.zeros(matrixNumWords, dimensions, dtype=torch.float)\n",
    "\n",
    "for i, word in enumerate(tweetField.vocab.itos):\n",
    "    wordVector = wordVectors.get(word)\n",
    "\n",
    "    if wordVector is not None:\n",
    "        embeddingsMatrix[i] = torch.from_numpy(wordVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAccuracy(predictedLabels: Iterable[Number], trueLabels: Iterable[Number]) -> float:    \n",
    "    correct = 0\n",
    "    for pred, true in zip(predictedLabels, trueLabels):\n",
    "        correct += int(pred == true)\n",
    "    \n",
    "    return correct/len(trueLabels)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictedLabels(predictions: torch.Tensor) -> np.ndarray:\n",
    "    softmaxLayerOut = torch.log_softmax(predictions, dim = 1)\n",
    "    _, labels = torch.max(softmaxLayerOut, dim = 1)\n",
    "    return labels.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipRNN(nn.Module):\n",
    "    def __init__(self, embeddingsMatrix, vectorDimension: int, numLayers: int, hiddenSize: int, skipConnections: bool = True) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.vectorDimension = vectorDimension\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddingsMatrix)\n",
    "        \n",
    "        self.startingLayer = nn.LSTM(   input_size=vectorDimension,\n",
    "                                        hidden_size=hiddenSize,\n",
    "                                        num_layers=1,\n",
    "                                        batch_first=True,\n",
    "                                        bidirectional=False )\n",
    "        self.lstmLayers = nn.ModuleList()\n",
    "        for _ in range(numLayers - 1):\n",
    "            self.lstmLayers.append(nn.LSTM( input_size=hiddenSize,\n",
    "                                            hidden_size=hiddenSize,\n",
    "                                            num_layers=1,\n",
    "                                            batch_first=True,\n",
    "                                            bidirectional=False))\n",
    "\n",
    "        self.linear = nn.Linear(hiddenSize, NUM_CLASSES)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        if (skipConnections):\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "    def forward(self, input, inputLengths):\n",
    "        embout = self.embedding(input)\n",
    "        #packedInput = pack_padded_sequence(embout, inputLengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        #nextInput, _ = pad_packed_sequence(self.startingLayer(packedInput))\n",
    "        nextInput, _ = self.startingLayer(embout)\n",
    "        nextSkipInput = nextInput\n",
    "        for layer in self.lstmLayers:\n",
    "            output, _ = layer(nextInput)\n",
    "            if nextSkipInput is not None:\n",
    "                nextInput = torch.add(output, self.skip(nextSkipInput))\n",
    "                nextInput = output\n",
    "                nextSkipInput = None\n",
    "            else:\n",
    "                nextSkipInput = output\n",
    "                nextInput = output\n",
    "        \n",
    "        #lstmOutput, _ = pad_packed_sequence(nextInput)\n",
    "        #lstmOutput = output\n",
    "        #return self.linear(lstmOutput)\n",
    "\n",
    "        return self.activation(self.linear(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipRNN(embeddingsMatrix, dimensions, numLayers=4, hiddenSize=32, skipConnections=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for ((tweets, tweetsLen), labels), _ in trainIterator:\n",
    "    out = model(tweets, tweetsLen)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFunction = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, F1 Score & Total Predictions after every epoch are stored here\n",
    "validPredictions: np.ndarray\n",
    "validF1: np.ndarray = np.empty(EPOCHS, dtype=float)\n",
    "validErrors: np.ndarray = np.empty(EPOCHS, dtype=float)\n",
    "validOutput: torch.Tensor\n",
    "\n",
    "trainF1: np.ndarray = np.empty(EPOCHS, dtype=float)\n",
    "trainErrors: np.ndarray = np.empty(EPOCHS, dtype=float)\n",
    "epochTrainPredictions: List[int]\n",
    "\n",
    "# Epochs loop\n",
    "for epoch in range(EPOCHS):\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    epochTrainPredictions = []\n",
    "    batchLosses = []\n",
    "    batchAccs = []\n",
    "\n",
    "    # Batch loop\n",
    "    for ((tweets, tweetsLen), labels), _ in trainIterator:\n",
    "        \n",
    "        # Make predictions for batch samples\n",
    "        predictions = model(tweets, tweetsLen)\n",
    "\n",
    "        # Extract & store predicted labels and calculate accuracy\n",
    "        predictedLabels = getPredictedLabels(predictions)\n",
    "        epochTrainPredictions.extend(predictedLabels)\n",
    "        batchAccs.append(calculateAccuracy(predictedLabels, labels))\n",
    "\n",
    "        # Run loss function, store loss & backpropagate\n",
    "        batchLoss = lossFunction(predictions, labels.long())\n",
    "        batchLosses.append(batchLoss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batchLoss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Find the total epoch loss & F1 Score for the Train set\n",
    "    trainErrors[epoch] = sum(batchLosses)/len(batchLosses)\n",
    "    trainF1[epoch] = f1_score(labels, epochTrainPredictions, average=\"micro\")\n",
    "    \n",
    "    print(f\"Epoch {epoch:3} Train Acc = {sum(batchAccs)/len(batchAccs):.5f}\", end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ((tweets, tweetsLen), labels), _ in trainIterator:\n",
    "    # tweets: Tensor with the tweets\n",
    "    # each tweet is a Tensor with the corresponding word indexes as values\n",
    "\n",
    "    # tweetsLen: Tensor, contains the length of the corresponding index tweet in `tweets`\n",
    "\n",
    "    embout = emb(tweets)\n",
    "    # embout: Tensor.\n",
    "    # Each element is a 2-D Tensor of a tweet. Each Tensor in it is the vector of the corresponding word.\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
