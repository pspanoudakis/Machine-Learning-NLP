{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from typing import Callable, Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 25\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET_PATH = \"./vaccine_train_set.csv\"\n",
    "VALIDATION_SET_PATH = \"./vaccine_validation_set.csv\"\n",
    "EMBEDDINGS_PATH = './glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVectors: \"dict[str, np.ndarray]\" = {}\n",
    "lineElements: \"list[str]\" = []\n",
    "with open(EMBEDDINGS_PATH) as file:\n",
    "    for line in file:\n",
    "        lineElements = line.replace('\\n', '').split()\n",
    "        word = lineElements.pop(0)\n",
    "        wordVector = np.array([float(w) for w in lineElements])\n",
    "        wordVectors[word] = wordVector\n",
    "dimensions = len(lineElements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "tweet         0\n",
       "label         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF = pd.read_csv(TRAIN_SET_PATH)\n",
    "trainDF.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "tweet         0\n",
       "label         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validDF = pd.read_csv(VALIDATION_SET_PATH)\n",
    "validDF.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = trainDF.drop(['label', 'Unnamed: 0'], axis=1)\n",
    "X_train = np.array([ar[0] for ar in features.values])\n",
    "trainLabels = trainDF['label'].values\n",
    "\n",
    "features = validDF.drop(['label', 'Unnamed: 0'], axis=1)\n",
    "X_valid = np.array([ar[0] for ar in features.values])\n",
    "validLabels = validDF['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customPreprocessor(text: str):    \n",
    "    # remove url's\n",
    "    trimmedText = re.sub(r'https?://\\S+|www\\.\\S+|#', '', text).lower()\n",
    "\n",
    "    # remove @ mentions and numbers\n",
    "    res = list()\n",
    "    wait_whitespace = False\n",
    "    for c in trimmedText:\n",
    "        if wait_whitespace:\n",
    "            if c == \" \":\n",
    "                wait_whitespace = False\n",
    "            continue\n",
    "        elif re.match(\"[0-9]\", c) or c == \"@\":\n",
    "            wait_whitespace = True\n",
    "            continue            \n",
    "        res.append(c)\n",
    "    \n",
    "    return ''.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeTweet(tweet: str, preprocessor: Callable[[str], str], wordVectors: \"dict[str, np.ndarray]\", dimensions: int) -> np.ndarray:\n",
    "    words = preprocessor(tweet).split()\n",
    "    vector: np.ndarray = np.zeros(dimensions)\n",
    "    for word in words:\n",
    "        wordVector = wordVectors.get(word)\n",
    "        if wordVector is not None:\n",
    "            vector += wordVector        \n",
    "    return vector / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet_X: np.ndarray = np.zeros(shape=(len(X_train), dimensions))\n",
    "for i, sample in enumerate(X_train):\n",
    "    trainSet_X[i] = vectorizeTweet(sample, customPreprocessor, wordVectors, dimensions)\n",
    "\n",
    "validSet_X: np.ndarray = np.zeros(shape=(len(X_valid), dimensions))\n",
    "for i, sample in enumerate(X_valid):\n",
    "    trainSet_X[i] = vectorizeTweet(sample, customPreprocessor, wordVectors, dimensions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, n_features: int, hidden_sizes: Iterable[int], n_classes: int) -> None:\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        n_in = n_features\n",
    "        for size in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(n_in, size))\n",
    "            n_in = size\n",
    "        self.layers.append(nn.Linear(n_in, n_classes))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        last_out = input\n",
    "        for layer in self.layers:\n",
    "            last_out = layer(last_out)\n",
    "        return last_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=50, out_features=64, bias=True)\n",
       "  (1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (3): Linear(in_features=16, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = Network(dimensions, [64, 32, 16], 3)\n",
    "network.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(trainLabels, dtype=torch.float)\n",
    "x = torch.tensor(trainSet_X, dtype=torch.float)\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "trainSetLoader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "y = torch.tensor(validLabels, dtype=torch.float)\n",
    "x = torch.tensor(validSet_X, dtype=torch.float)\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "validSetLoader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  49: Loss = 0.98904\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    batchLosses = []\n",
    "\n",
    "    for batchSamples, batchLabels in trainSetLoader:\n",
    "        predictions = network(batchSamples)\n",
    "\n",
    "        batchLoss = lossFunction(predictions, batchLabels.long())\n",
    "        batchLosses.append(batchLoss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batchLoss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch:3}: Loss = {sum(batchLosses)/len(trainSetLoader):.5f}\\r\", end='')\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2282) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8012/2553260442.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmulti_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8012/3104480955.py\u001b[0m in \u001b[0;36mmulti_acc\u001b[0;34m(y_pred, y_test)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_softmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcorrect_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred_tags\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2282) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "pred = network(x)\n",
    "\n",
    "multi_acc(pred, y.long())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
