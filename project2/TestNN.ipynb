{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from typing import Callable, Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET_PATH = \"./vaccine_train_set.csv\"\n",
    "VALIDATION_SET_PATH = \"./vaccine_validation_set.csv\"\n",
    "EMBEDDINGS_PATH = './glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVectors: \"dict[str, np.ndarray]\" = {}\n",
    "lineElements: \"list[str]\" = []\n",
    "with open(EMBEDDINGS_PATH) as file:\n",
    "    for line in file:\n",
    "        lineElements = line.replace('\\n', '').split()\n",
    "        word = lineElements.pop(0)\n",
    "        wordVector = np.array([float(w) for w in lineElements])\n",
    "        wordVectors[word] = wordVector\n",
    "dimensions = len(lineElements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "tweet         0\n",
       "label         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF = pd.read_csv(TRAIN_SET_PATH)\n",
    "trainDF.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "tweet         0\n",
       "label         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validDF = pd.read_csv(VALIDATION_SET_PATH)\n",
    "validDF.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = trainDF.drop(['label', 'Unnamed: 0'], axis=1)\n",
    "X_train = np.array([ar[0] for ar in features.values])\n",
    "trainLabels = trainDF['label'].values\n",
    "\n",
    "features = validDF.drop(['label', 'Unnamed: 0'], axis=1)\n",
    "X_valid = np.array([ar[0] for ar in features.values])\n",
    "validLabels = validDF['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customPreprocessor(text: str):    \n",
    "    # remove url's\n",
    "    trimmedText = re.sub(r'https?://\\S+|www\\.\\S+|#', '', text).lower()\n",
    "\n",
    "    # remove @ mentions and numbers\n",
    "    res = list()\n",
    "    wait_whitespace = False\n",
    "    for c in trimmedText:\n",
    "        if wait_whitespace:\n",
    "            if c == \" \":\n",
    "                wait_whitespace = False\n",
    "            continue\n",
    "        elif re.match(\"[0-9]\", c) or c == \"@\":\n",
    "            wait_whitespace = True\n",
    "            continue            \n",
    "        res.append(c)\n",
    "    \n",
    "    return ''.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeTweet(tweet: str, preprocessor: Callable[[str], str], wordVectors: \"dict[str, np.ndarray]\", dimensions: int) -> np.ndarray:\n",
    "    words = preprocessor(tweet).split()\n",
    "    vector: np.ndarray = np.zeros(dimensions)\n",
    "    for word in words:\n",
    "        wordVector = wordVectors.get(word)\n",
    "        if wordVector is not None:\n",
    "            vector += wordVector        \n",
    "    return vector / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet_X: np.ndarray = np.zeros(shape=(len(X_train), dimensions))\n",
    "for i, sample in enumerate(X_train):\n",
    "    trainSet_X[i] = vectorizeTweet(sample, customPreprocessor, wordVectors, dimensions)\n",
    "\n",
    "validSet_X: np.ndarray = np.zeros(shape=(len(X_valid), dimensions))\n",
    "for i, sample in enumerate(X_valid):\n",
    "    validSet_X[i] = vectorizeTweet(sample, customPreprocessor, wordVectors, dimensions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, n_features: int, hidden_sizes: Iterable[int], n_classes: int) -> None:\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        n_in = n_features\n",
    "        for size in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(n_in, size))\n",
    "            n_in = size\n",
    "        self.layers.append(nn.Linear(n_in, n_classes))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        last_out = input\n",
    "        for layer in self.layers:\n",
    "            last_out = layer(last_out)\n",
    "        return last_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=50, out_features=32, bias=True)\n",
       "  (1): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (2): Linear(in_features=16, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = Network(dimensions, [32, 16], NUM_CLASSES)\n",
    "network.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createClassWeights(sampleLabels, numClasses):\n",
    "    classCounts = [0 for n in range(numClasses)]\n",
    "    for label in sampleLabels:\n",
    "        classCounts[label] += 1\n",
    "\n",
    "    weights = []\n",
    "    for count in classCounts:\n",
    "        weights.append(len(sampleLabels)/count)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(trainLabels, dtype=torch.float)\n",
    "x = torch.tensor(trainSet_X, dtype=torch.float)\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "trainSetLoader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "y = torch.tensor(validLabels, dtype=torch.float)\n",
    "x = torch.tensor(validSet_X, dtype=torch.float)\n",
    "\n",
    "weights = createClassWeights(trainLabels, NUM_CLASSES)\n",
    "sampler = WeightedRandomSampler(weights, len(trainLabels))\n",
    "dataset = TensorDataset(x, y)\n",
    "validSetLoader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, sampler=sampler)\n",
    "\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAccuracy(y_pred, y_true):\n",
    "    softmaxLayerOut = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, predictions = torch.max(softmaxLayerOut, dim = 1)\n",
    "    \n",
    "    correct = 0\n",
    "    for pred, true in zip(predictions, y_true):\n",
    "        correct += int(pred == true)\n",
    "    \n",
    "    return correct/len(y_true)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_230/2622428038.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbatchLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchLabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mbatchAccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculateAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mbatchLosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_230/158198480.py\u001b[0m in \u001b[0;36mcalculateAccuracy\u001b[0;34m(y_pred, y_true)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    network.train()\n",
    "    batchLosses = []\n",
    "    batchAccs = []\n",
    "\n",
    "    for batchSamples, batchLabels in trainSetLoader:\n",
    "        predictions = network(batchSamples)\n",
    "\n",
    "        batchLoss = lossFunction(predictions, batchLabels.long())\n",
    "        batchAccs.append(calculateAccuracy(predictions, batchLabels))\n",
    "        batchLosses.append(batchLoss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batchLoss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    network.eval()\n",
    "    pred = network(x)\n",
    "    acc = calculateAccuracy(pred, y)\n",
    "    #predictions = [list(pred) for pred in network(x).detach().numpy()]\n",
    "    #predictions = [pred.index(max(pred)) for pred in predictions]\n",
    "    #score = f1_score(y.detach().numpy(), predictions, average=\"macro\")\n",
    "    #print(f\"Epoch {epoch:3}: Loss = {sum(batchLosses)/len(trainSetLoader):.5f} Validation Score: {score}\\r\", end='')\n",
    "    print(f\"Epoch {epoch:3} Train Acc = {sum(batchAccs)/len(batchAccs):.5f} Valid Acc = {acc:.5f}\\r\", end='')\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
